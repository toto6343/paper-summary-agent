# PDF ë…¼ë¬¸ ìš”ì•½ Agent - msgpack ì§ë ¬í™” ì˜¤ë¥˜ ìˆ˜ì • ë²„ì „
# requirements.txtì— ë‹¤ìŒ íŒ¨í‚¤ì§€ë“¤ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”:
# langchain
# langgraph  
# langchain-openai
# langchain-chroma
# pypdf
# faiss-cpu
# streamlit
# python-dotenv

import os
import tempfile
import pickle
from typing import List, Dict, Any, TypedDict, Annotated, Optional
from pathlib import Path

# dotenv í™˜ê²½ë³€ìˆ˜ ë¡œë”©
from dotenv import load_dotenv
load_dotenv()  # .env íŒŒì¼ ë¡œë”©

# LangChain imports
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.tools import tool
from langchain.schema import Document
from langchain_community.vectorstores import FAISS

# LangGraph imports
from langgraph.graph import StateGraph, END
from langgraph.graph.message import add_messages

# =============================================================================
# 1ë‹¨ê³„: ìƒíƒœ ì •ì˜ (ë²¡í„° ìŠ¤í† ì–´ ì§ë ¬í™” ë¬¸ì œ í•´ê²°)
# =============================================================================

class AgentState(TypedDict):
    """Agentì˜ ìƒíƒœë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ - ì§ë ¬í™” ê°€ëŠ¥í•œ ë°ì´í„°ë§Œ í¬í•¨"""
    messages: Annotated[list, add_messages]
    pdf_content: str
    chunks: List[str]  # ë²¡í„° ìŠ¤í† ì–´ ëŒ€ì‹  ì²­í¬ë¥¼ ì €ì¥
    summary: str
    qa_history: List[Dict]
    current_query: str
    vector_store_path: str  # ë²¡í„° ìŠ¤í† ì–´ ê²½ë¡œë§Œ ì €ì¥

# =============================================================================
# 2ë‹¨ê³„: ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬ í´ë˜ìŠ¤
# =============================================================================

class VectorStoreManager:
    """ë²¡í„° ìŠ¤í† ì–´ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.vector_stores = {}  # ë©”ëª¨ë¦¬ì— ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
        
    def create_vector_store(self, chunks: List[str], store_id: str = "default") -> str:
        """ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ID ë°˜í™˜"""
        try:
            embeddings = OpenAIEmbeddings()
            documents = [Document(page_content=chunk) for chunk in chunks]
            
            # FAISS ì‚¬ìš© (ë©”ëª¨ë¦¬ ë‚´ ì €ì¥ì´ ë” ì•ˆì •ì )
            vector_store = FAISS.from_documents(
                documents=documents,
                embedding=embeddings
            )
            
            # ë©”ëª¨ë¦¬ì— ì €ì¥
            self.vector_stores[store_id] = vector_store
            return store_id
            
        except Exception as e:
            print(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return ""
    
    def get_vector_store(self, store_id: str):
        """ë²¡í„° ìŠ¤í† ì–´ ì¡°íšŒ"""
        return self.vector_stores.get(store_id)
    
    def search(self, store_id: str, query: str, k: int = 3) -> List[str]:
        """ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰"""
        vector_store = self.get_vector_store(store_id)
        if vector_store:
            docs = vector_store.similarity_search(query, k=k)
            return [doc.page_content for doc in docs]
        return []

# ì „ì—­ ë²¡í„° ìŠ¤í† ì–´ ë§¤ë‹ˆì €
vector_manager = VectorStoreManager()

# =============================================================================
# 3ë‹¨ê³„: ë‹¨ìˆœí™”ëœ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# =============================================================================

def load_and_chunk_pdf(pdf_path: str) -> tuple[str, List[str]]:
    """PDF ë¡œë“œ ë° ì²­í‚¹"""
    try:
        # PDF ë¡œë“œ
        loader = PyPDFLoader(pdf_path)
        pages = loader.load()
        content = "\n\n".join([page.page_content for page in pages])
        
        # í…ìŠ¤íŠ¸ ì²­í‚¹
        chunk_size = int(os.getenv("CHUNK_SIZE", 1000))
        overlap = int(os.getenv("CHUNK_OVERLAP", 200))
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=overlap,
            separators=["\n\n", "\n", ". ", " "]
        )
        chunks = text_splitter.split_text(content)
        
        return content, chunks
        
    except Exception as e:
        print(f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        return f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}", []

def generate_summary(content: str, summary_type: str = "comprehensive") -> str:
    """ë…¼ë¬¸ ìš”ì•½ ìƒì„±"""
    try:
        if not content or len(content.strip()) < 10:
            return "ìš”ì•½í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."
        
        print(f"ìš”ì•½ ìƒì„± ì‹œì‘: {len(content)} ë¬¸ì")
        
        model_name = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
        
        # API í‚¤ í™•ì¸
        if not os.getenv("OPENAI_API_KEY"):
            return "ìš”ì•½ ìƒì„± ì˜¤ë¥˜: OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        llm = ChatOpenAI(model=model_name, temperature=0)
        
        # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
        max_length = 8000 if summary_type == "comprehensive" else 5000
        content_for_summary = content[:max_length]
        if len(content) > max_length:
            content_for_summary += "\n\n[ë‚´ìš©ì´ ê¸¸ì–´ ì¼ë¶€ë§Œ ìš”ì•½ì— ì‚¬ìš©ë¨]"
        
        if summary_type == "brief":
            prompt = f"""
ë‹¤ìŒ ë…¼ë¬¸ì„ 3-5ì¤„ë¡œ ê°„ë‹¨íˆ ìš”ì•½í•´ì£¼ì„¸ìš”:

{content_for_summary}

ìš”ì•½:
"""
        elif summary_type == "structured":
            prompt = f"""
ë‹¤ìŒ ë…¼ë¬¸ì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

{content_for_summary}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”:
1. ì œëª© ë° ì €ì
2. ì—°êµ¬ ëª©ì 
3. ì£¼ìš” ë°©ë²•ë¡ 
4. í•µì‹¬ ê²°ê³¼
5. ê²°ë¡  ë° ì˜ì˜
"""
        else:  # comprehensive
            prompt = f"""
ë‹¤ìŒ ë…¼ë¬¸ì„ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:

{content_for_summary}

í¬í•¨í•  ë‚´ìš©:
- ì—°êµ¬ ë°°ê²½ ë° ë™ê¸°
- ì—°êµ¬ ë¬¸ì œ ë° ê°€ì„¤
- ë°©ë²•ë¡  ë° ì‹¤í—˜ ì„¤ê³„
- ì£¼ìš” ê²°ê³¼ ë° ë°œê²¬ì‚¬í•­
- ë…¼ì˜ ë° ì œí•œì 
- ê²°ë¡  ë° í–¥í›„ ì—°êµ¬ ë°©í–¥
"""
        
        print("OpenAI API í˜¸ì¶œ ì‹œì‘")
        response = llm.invoke(prompt)
        print("ìš”ì•½ ìƒì„± ì™„ë£Œ")
        
        return response.content
        
    except Exception as e:
        error_msg = f"ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {str(e)}"
        print(error_msg)
        import traceback
        print(traceback.format_exc())
        return error_msg

def answer_question(question: str, vector_store_id: str, chat_history: List = None) -> str:
    """ì§ˆë¬¸ ë‹µë³€ ìƒì„±"""
    try:
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        search_k = int(os.getenv("SEARCH_K", 3))
        context_docs = vector_manager.search(vector_store_id, question, k=search_k)
        context = "\n\n".join(context_docs)
        
        if not context:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
        model_name = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
        llm = ChatOpenAI(model=model_name, temperature=0)
        
        history_context = ""
        if chat_history:
            history_limit = int(os.getenv("HISTORY_LIMIT", 3))
            recent_history = chat_history[-history_limit:]
            history_context = "\nì´ì „ ëŒ€í™”:\n" + "\n".join([
                f"Q: {item['question']}\nA: {item['answer']}" 
                for item in recent_history
            ])
        
        prompt = f"""
        ë‹¤ìŒ ë…¼ë¬¸ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:
        
        ê´€ë ¨ ë‚´ìš©:
        {context}
        
        {history_context}
        
        ì§ˆë¬¸: {question}
        
        ë‹µë³€:
        """
        
        response = llm.invoke(prompt)
        return response.content
        
    except Exception as e:
        return f"ì§ˆë¬¸ ë‹µë³€ ì˜¤ë¥˜: {str(e)}"

# =============================================================================
# 4ë‹¨ê³„: ë‹¨ìˆœí™”ëœ ë´‡ í´ë˜ìŠ¤ (LangGraph ì—†ì´)
# =============================================================================

class PaperSummaryBot:
    def __init__(self):
        self.pdf_content = ""
        self.summary = ""
        self.chat_history = []
        self.vector_store_id = ""
        
    def process_pdf(self, pdf_file):
        """PDF íŒŒì¼ ì²˜ë¦¬"""
        if pdf_file is None:
            return "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        
        try:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                # Streamlit UploadedFile ê°ì²´ ì²˜ë¦¬
                if hasattr(pdf_file, 'read'):
                    tmp_file.write(pdf_file.read())
                else:
                    tmp_file.write(pdf_file.getvalue())
                tmp_path = tmp_file.name
            
            # PDF ë¡œë“œ ë° ì²­í‚¹
            content, chunks = load_and_chunk_pdf(tmp_path)
            
            if not chunks:
                return "PDF ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            
            # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
            import uuid
            store_id = str(uuid.uuid4())
            vector_store_id = vector_manager.create_vector_store(chunks, store_id)
            
            if not vector_store_id:
                return "ë²¡í„° ìŠ¤í† ì–´ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            
            # ìš”ì•½ ìƒì„±
            summary_type = os.getenv("SUMMARY_TYPE", "comprehensive")
            summary = generate_summary(content, summary_type)
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            self.pdf_content = content
            self.summary = summary
            self.vector_store_id = vector_store_id
            self.chat_history = []
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.unlink(tmp_path)
            
            return summary
            
        except Exception as e:
            return f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def ask_question(self, question: str) -> str:
        """ì§ˆë¬¸ ë‹µë³€"""
        if not question.strip():
            return "ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
        
        if not self.vector_store_id:
            return "ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”."
        
        try:
            answer = answer_question(question, self.vector_store_id, self.chat_history)
            
            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.chat_history.append({
                "question": question,
                "answer": answer
            })
            
            return answer
            
        except Exception as e:
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"

# =============================================================================
# í™˜ê²½ë³€ìˆ˜ ì²´í¬ í•¨ìˆ˜
# =============================================================================

def check_environment():
    """í™˜ê²½ë³€ìˆ˜ ì„¤ì • í™•ì¸"""
    if not os.getenv("OPENAI_API_KEY"):
        return False
    return True

# =============================================================================
# Streamlit UI
# =============================================================================

def main():
    """Streamlit ë©”ì¸ í•¨ìˆ˜"""
    
    try:
        import streamlit as st
        
        st.set_page_config(
            page_title="ğŸ“„ PDF ë…¼ë¬¸ ìš”ì•½ Agent",
            page_icon="ğŸ“„",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
        # íƒ€ì´í‹€
        st.title("ğŸ“„ PDF ë…¼ë¬¸ ìš”ì•½ Agent")
        st.markdown("PDF ë…¼ë¬¸ì„ ì—…ë¡œë“œí•˜ê³  ìš”ì•½ì„ ìƒì„±í•˜ê±°ë‚˜ ì§ˆë¬¸í•˜ì„¸ìš”!")
        
        # API í‚¤ í™•ì¸
        if not os.getenv("OPENAI_API_KEY"):
            st.error("ğŸš¨ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            
            with st.expander("ğŸ”§ í™˜ê²½ ì„¤ì • ë°©ë²•", expanded=True):
                st.info("í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— .env íŒŒì¼ì„ ìƒì„±í•˜ê³  API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
                st.code("OPENAI_API_KEY=your-api-key-here")
                
                st.markdown("### ì¶”ê°€ ì„¤ì • (ì„ íƒì‚¬í•­)")
                st.code("""OPENAI_MODEL=gpt-3.5-turbo
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
SEARCH_K=3
HISTORY_LIMIT=3
SUMMARY_TYPE=comprehensive""")
            
            st.stop()
        
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        if 'bot' not in st.session_state:
            st.session_state.bot = PaperSummaryBot()
            
        if 'summary_displayed' not in st.session_state:
            st.session_state.summary_displayed = ""
        
        # ì‚¬ì´ë“œë°” - PDF ì—…ë¡œë“œ
        with st.sidebar:
            st.header("ğŸ“„ PDF ì—…ë¡œë“œ")
            uploaded_file = st.file_uploader(
                "PDF íŒŒì¼ ì„ íƒ",
                type=['pdf'],
                help="ë¶„ì„í•  PDF ë…¼ë¬¸ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
            )
            
            if uploaded_file and st.button("ğŸ“ ìš”ì•½ ìƒì„±", type="primary"):
                with st.spinner("ë…¼ë¬¸ ë¶„ì„ ì¤‘..."):
                    try:
                        summary = st.session_state.bot.process_pdf(uploaded_file)
                        st.session_state.summary_displayed = summary
                        st.success("âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ!")
                        st.rerun()
                    except Exception as e:
                        st.error(f"âŒ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            
            # ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
            if st.session_state.bot.chat_history and st.button("ğŸ”„ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
                st.session_state.bot.chat_history = []
                st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                st.rerun()
        
        # ë©”ì¸ ì˜ì—­
        if st.session_state.summary_displayed:
            st.markdown("### ğŸ“„ ë…¼ë¬¸ ìš”ì•½ ê²°ê³¼")
            st.markdown(st.session_state.summary_displayed)
            
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            st.download_button(
                label="ğŸ“¥ ìš”ì•½ ë‹¤ìš´ë¡œë“œ (TXT)",
                data=st.session_state.summary_displayed,
                file_name="paper_summary.txt",
                mime="text/plain"
            )
            
            st.divider()
            
            # ì§ˆë¬¸ ì„¹ì…˜
            st.markdown("### ğŸ’­ ì§ˆë¬¸í•˜ê¸°")
            
            # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
            if st.session_state.bot.chat_history:
                st.markdown("#### ëŒ€í™” ê¸°ë¡")
                for i, qa in enumerate(st.session_state.bot.chat_history):
                    with st.container():
                        st.markdown(f"**Q{i+1}:** {qa['question']}")
                        st.markdown(f"**A{i+1}:** {qa['answer']}")
                        if i < len(st.session_state.bot.chat_history) - 1:
                            st.divider()
                
                st.divider()
            
            # ì§ˆë¬¸ ì…ë ¥
            with st.form("question_form", clear_on_submit=True):
                question = st.text_area(
                    "ë…¼ë¬¸ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”:",
                    placeholder="ì˜ˆ: ì´ ë…¼ë¬¸ì˜ ì£¼ìš” ê¸°ì—¬ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                    height=100
                )
                submitted = st.form_submit_button("ì§ˆë¬¸í•˜ê¸°", type="primary")
                
                if submitted and question.strip():
                    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                        try:
                            answer = st.session_state.bot.ask_question(question)
                            st.success("âœ… ë‹µë³€ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                            st.rerun()
                        except Exception as e:
                            st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        else:
            st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ PDFë¥¼ ì—…ë¡œë“œí•˜ê³  ìš”ì•½ ìƒì„± ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
            
            # ì‚¬ìš©ë²• ì•ˆë‚´
            with st.expander("ğŸ“‹ ì‚¬ìš©ë²• ì•ˆë‚´", expanded=True):
                st.markdown("""
                ### ğŸ“‹ ì‚¬ìš© ë°©ë²•
                1. **PDF ì—…ë¡œë“œ**: ì‚¬ì´ë“œë°”ì—ì„œ PDF ë…¼ë¬¸ íŒŒì¼ì„ ì—…ë¡œë“œ
                2. **ìš”ì•½ ìƒì„±**: 'ìš”ì•½ ìƒì„±' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë…¼ë¬¸ ìš”ì•½ ìƒì„±
                3. **ì§ˆë¬¸í•˜ê¸°**: ìš”ì•½ ìƒì„± í›„ ë…¼ë¬¸ì— ëŒ€í•œ ì§ˆë¬¸ ì…ë ¥
                4. **ë‹µë³€ í™•ì¸**: AIê°€ ìƒì„±í•œ ë‹µë³€ì„ í™•ì¸
                
                ### ğŸ”§ ì£¼ìš” ê¸°ëŠ¥
                - PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹
                - FAISS ë²¡í„° ìŠ¤í† ì–´ë¥¼ í†µí•œ ì˜ë¯¸ì  ê²€ìƒ‰
                - LLM ê¸°ë°˜ ë…¼ë¬¸ ìš”ì•½
                - ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€
                - ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬
                
                ### âš™ï¸ í˜„ì¬ ì„¤ì •
                - **ëª¨ë¸**: {os.getenv('OPENAI_MODEL', 'gpt-3.5-turbo')}
                - **ì²­í¬ í¬ê¸°**: {os.getenv('CHUNK_SIZE', '1000')}
                - **ê²€ìƒ‰ ê²°ê³¼ ìˆ˜**: {os.getenv('SEARCH_K', '3')}
                """)
    
    except ImportError as e:
        print(f"âŒ ëª¨ë“ˆ import ì˜¤ë¥˜: {str(e)}")
        print("í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install streamlit")
    except Exception as e:
        print(f"âŒ Streamlit ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")

# =============================================================================
# ë©”ì¸ ì‹¤í–‰ë¶€
# =============================================================================

if __name__ == "__main__":
    # í™˜ê²½ë³€ìˆ˜ ì²´í¬
    if not check_environment():
        print("\nğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— .env íŒŒì¼ì„ ìƒì„±í•˜ê³  ë‹¤ìŒ ë‚´ìš©ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”:")
        print("OPENAI_API_KEY=your-api-key-here")
        exit(1)
    
    # Streamlitìœ¼ë¡œ ì‹¤í–‰
    main()